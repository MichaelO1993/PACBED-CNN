{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lyric-weather",
   "metadata": {},
   "source": [
    "# CNN-Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fitted-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aging-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Parameters - CNN-Training\n",
    "parameters_training = {\n",
    "    'reg_id' : 0,  # Number of the register entry\n",
    "    'conv_range' : None,             # [mrad] Convergence angle range, for which models should be trained, None -> no filtering\n",
    "    'thickness_range' : None        # [nm]   Thickness range, for which models should be trained, None -> no filtering\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hollywood-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Snippet limits GPU memory growth -> without, errors occur (may not necessary for cluster/other computers)\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=\n",
    "                                  tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "                                  # device_count = {'GPU': 1}\n",
    "                                  )\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "atmospheric-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_PACBED_Trainer:\n",
    "    def __init__(self, parameters):\n",
    "        # Declare variables\n",
    "        self.Reg_id = parameters['reg_id']\n",
    "\n",
    "        self.df = None\n",
    "        self.df_filtered = None\n",
    "        self.df_train = None\n",
    "        self.df_validation = None\n",
    "        self.dim = None\n",
    "        self.rescaling = None\n",
    "        \n",
    "        # Get register entry\n",
    "        path = self.make_register()\n",
    "        \n",
    "        # Load dataframe\n",
    "        self.df = pd.read_csv(os.path.join(path, 'simulation', 'df.csv'), sep = ';')\n",
    "        \n",
    "    def make_register(self,):\n",
    "        # Load register file\n",
    "        df_register = pd.read_csv('.\\\\data\\\\Register.csv', sep = ';', index_col = 'id')\n",
    "        \n",
    "        # Get parameters\n",
    "        df_system = df_register.loc[self.Reg_id]\n",
    "        print(f'Loaded system: \\n')\n",
    "        display(df_register.loc[[self.Reg_id]])\n",
    "       \n",
    "        self.path_models = os.path.join(df_system.loc['path'], 'models')\n",
    "        # Check if already trained model is present\n",
    "        if not os.path.exists(self.path_models):\n",
    "            # Make path for saving models\n",
    "            os.mkdir(self.path_models)\n",
    "            \n",
    "            # Make register for the models\n",
    "            column_names = ['path', 'thickness', 'convergenc angle min', 'convergenc angle max']\n",
    "            self.df_register_model = pd.DataFrame(columns = column_names)\n",
    "\n",
    "            self.reg_id_model = 0\n",
    "            \n",
    "        else:\n",
    "            self.df_register_model = pd.read_csv(os.path.join(self.path_models, 'Register_models.csv'), sep = ';', index_col = 'id')\n",
    "            \n",
    "            self.reg_id_model = np.amax(self.df_register_model.index) + 1\n",
    "            \n",
    "        self.path_model = os.path.join(self.path_models, str(self.reg_id_model))\n",
    "        \n",
    "        os.mkdir(self.path_model)    \n",
    "            \n",
    "        # Save entry\n",
    "        data_entry = [self.path_model, df_system.loc['thickness'], df_system.loc['convergenc angle min'], df_system.loc['convergenc angle max']]\n",
    "        df_model = pd.DataFrame(data = [data_entry], columns = self.df_register_model.columns, index = [self.reg_id_model])\n",
    "        df_model.index.name = 'id'\n",
    "        self.df_register_model = pd.concat([self.df_register_model, df_model])\n",
    "        self.df_register_model.to_csv(os.path.join(self.path_models, 'Register_models.csv'), sep = ';', index = True, index_label = 'id')\n",
    "        \n",
    "        return df_system.loc['path']\n",
    "            \n",
    "    \n",
    "    # Filter trainings dataset to conv_range and thickness_range\n",
    "    def filter_dataset(self, conv_range = None, thickness_range = None):\n",
    "        if conv_range == None:\n",
    "            self.df_filtered = self.df\n",
    "            print('Model will be trained from ({:.1f} to {:.1f}) mrad convergence angle.'.format(np.amin(self.df['Conv_Angle']), np.amax(self.df['Conv_Angle'])))\n",
    "        else:\n",
    "            self.df_filtered = self.df[(self.df['Conv_Angle'] >= np.amin(conv_range)) & (self.df['Conv_Angle'] <= np.amax(conv_range))]\n",
    "            print('Model will be trained from ({:.1f} to {:.1f}) mrad convergence angle.'.format(np.amin(self.df_filtered['Conv_Angle']), np.amax(self.df_filtered['Conv_Angle'])))\n",
    "        \n",
    "        if thickness_range == None:\n",
    "            print('Model will be trained from ({:.1f} to {:.1f}) nm thickness.'.format(np.amin(self.df['Thickness'])/10, np.amax(self.df['Thickness']/10)))\n",
    "        else:\n",
    "            self.df_filtered = self.df[(self.df_filtered['Thickness'] >= 10*np.amin(thickness_range)) & (self.df_filtered['Thickness'] <= 10*np.amax(thickness_range))]\n",
    "            print('Model will be trained from ({:.1f} to {:.1f}) nm thickness.'.format(np.amin(self.df_filtered['Thickness'])/10, np.amax(self.df_filtered['Thickness']/10)))\n",
    "\n",
    "        self.df_filtered.reset_index()\n",
    "        \n",
    "        # Preparing Dataframe for training and validation\n",
    "        splitting_ratio = 0.05\n",
    "        seed = 42\n",
    "        self.df_train, self.df_validation = train_test_split(self.df_filtered, test_size=splitting_ratio, random_state = seed)\n",
    "        \n",
    "        print('Number of images: {}'.format(len(self.df_filtered)))\n",
    "        print('{:.0f}% used for training, {:.0f}% used for validation'.format((1-splitting_ratio)*100, splitting_ratio*100))\n",
    "        \n",
    "        # Update model register file\n",
    "        self.df_register_model.loc[self.df_register_model.index == self.reg_id_model, 'convergenc angle min'] = np.amin(self.df_filtered['Conv_Angle'])\n",
    "        self.df_register_model.loc[self.df_register_model.index == self.reg_id_model, 'convergenc angle max'] = np.amax(self.df_filtered['Conv_Angle'])\n",
    "        self.df_register_model.loc[self.df_register_model.index == self.reg_id_model, 'thickness'] = np.amax(self.df_filtered['Thickness'])/10\n",
    "\n",
    "        self.df_register_model.to_csv(os.path.join(self.path_models, 'Register_models.csv'), sep = ';', index = True, index_label = 'id')\n",
    "        \n",
    "        \n",
    "    # Define dimension of CNN input\n",
    "    def CNN_dim(self, dim = (0,0,0)):\n",
    "        # If no dim input given, an example image will be loaded and used for dimension determination --> no rescaling required\n",
    "        if dim == (0,0,0):\n",
    "            self.dim = np.array(Image.open(self.df['Path'][0])).shape\n",
    "            self.rescaling = False\n",
    "        else:\n",
    "            self.dim = dim\n",
    "            self.rescaling = True\n",
    "            \n",
    "        # Convert dimension to rgb (required for pretrained models), otherwise grayscale can be used also\n",
    "        if len(self.dim) == 3:\n",
    "            self.dim = list(self.dim)\n",
    "            self.dim[-1] = 3\n",
    "        else:\n",
    "            self.dim = self.dim + (3,)\n",
    "        print('Shape of CNN input: {}'.format(self.dim))\n",
    "    \n",
    "    def build_model(self, n_classes, fc_layers = [512, 512], dropout = 0.3):\n",
    "        \n",
    "        # Load pretrainerd xception model (can be changed with other models or non-pretrained model)\n",
    "        base_model = tf.keras.applications.Xception(weights = 'imagenet',\n",
    "                                                    include_top = False,\n",
    "                                                    pooling = 'avg', # max pooling may perform better or worse\n",
    "                                                    input_shape = self.dim)\n",
    "        # Build model\n",
    "        \n",
    "        # Inputs\n",
    "        inputs_img = tf.keras.Input(shape=self.dim) # Image\n",
    "        input_conv = tf.keras.Input(shape=(1))      # Cnvergence angle\n",
    "\n",
    "        # Pretrained model\n",
    "        x = base_model(inputs_img)\n",
    "\n",
    "        # Build up fully connected layers with dropout\n",
    "        for fc in fc_layers:\n",
    "            x = tf.keras.layers.Dense(fc, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        # Add second input, convergence angel\n",
    "        x = tf.keras.layers.Concatenate(axis=1)([x, input_conv])\n",
    "\n",
    "        # Number of putputs defined by the datagenerator\n",
    "        outputs = tf.keras.layers.Dense(n_classes, activation='softmax')(x)\n",
    "        \n",
    "        # Full model\n",
    "        model = tf.keras.Model(inputs = (inputs_img, input_conv), outputs = outputs)        \n",
    "        print(model.summary())\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def CNN_training(self, model, datagenerator_train, datagenerator_validation, epochs, path):\n",
    "    \n",
    "        # Callback functions for saving the model with the lowest validation loss --> used for final model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=path,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "\n",
    "        # Compile  model\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(),\n",
    "                      metrics=[tf.keras.metrics.CategoricalAccuracy()]) \n",
    "\n",
    "        # Train model\n",
    "        history_train = model.fit(datagenerator_train,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=datagenerator_validation,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                                  )\n",
    "\n",
    "        return history_train\n",
    "    \n",
    "    \n",
    "    def train_thickness(self, epochs = 40, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0):\n",
    "        # Generate labels\n",
    "        label_thickness, scale_vec = self.label_gen('Thickness', self.df_filtered)\n",
    "        \n",
    "        # Initialize datagenerator\n",
    "        datagenerator_thickness_train = self.DataGenerator(self.df_train, label_thickness, 'Thickness', self.rescaling, self.dim, batch_size, shuffle=True, scale_vec=scale_vec)\n",
    "        datagenerator_thickness_validation = self.DataGenerator(self.df_validation, label_thickness, 'Thickness', self.rescaling, self.dim, batch_size, shuffle=False, scale_vec=scale_vec)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(self.path_model, 'Thickness')\n",
    "        \n",
    "        # Generate model\n",
    "        model_thickness = self.build_model(n_classes = len(label_thickness))\n",
    "        \n",
    "        if not transfer_learning:\n",
    "            # Generate model\n",
    "            model_thickness = self.build_model(n_classes = len(label_thickness))\n",
    "\n",
    "            # Train model\n",
    "            history_thickness_train = self.CNN_training(model_thickness, datagenerator_thickness_train, datagenerator_thickness_validation, epochs, checkpoint_filepath)\n",
    "        else:\n",
    "            history_thickness_train = self.ft_training(reg_id, model_id, 'Thickness', datagenerator_thickness_train, datagenerator_thickness_validation, checkpoint_filepath, len(label_thickness),lr_base = 0.001, epochs_base = 1, epochs_ft = 10)\n",
    "                  \n",
    "        # Transform tensorflow SavedModel to tensorflow lite models\n",
    "        self.transform_model(checkpoint_filepath, delete_checkpoint)\n",
    "        \n",
    "        print('Training finished!')\n",
    "        \n",
    "    def train_mistilt(self, epochs = 20, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0):\n",
    "        # Generate labels\n",
    "        label_mistilt, scale_vec = self.label_gen('Mistilt', self.df_filtered)\n",
    "        \n",
    "        # Initialize datagenerator\n",
    "        datagenerator_mistilt_train = self.DataGenerator(self.df_train, label_mistilt, 'Mistilt', self.rescaling, self.dim, batch_size, shuffle=True, scale_vec=scale_vec)\n",
    "        datagenerator_mistilt_validation = self.DataGenerator(self.df_validation, label_mistilt, 'Mistilt', self.rescaling, self.dim, batch_size, shuffle=False, scale_vec=scale_vec)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(self.path_model, 'Mistilt')\n",
    "        \n",
    "        # Generate model\n",
    "        model_mistilt = self.build_model(n_classes = len(label_mistilt))\n",
    "        \n",
    "        if not transfer_learning:\n",
    "            # Generate model\n",
    "            model_mistilt = self.build_model(n_classes = len(label_mistilt))\n",
    "\n",
    "            # Train model\n",
    "            history_mistilt_train = self.CNN_training(model_mistilt, datagenerator_mistilt_train, datagenerator_mistilt_validation, epochs, checkpoint_filepath)\n",
    "        else:\n",
    "            history_mistilt_train = self.ft_training(reg_id, model_id, 'Mistilt', datagenerator_mistilt_train, datagenerator_mistilt_validation, checkpoint_filepath, len(label_mistilt),lr_base = 0.001, epochs_base = 1, epochs_ft = 10)\n",
    "          \n",
    "        # Transform tensorflow SavedModel to tensorflow lite models\n",
    "        self.transform_model(checkpoint_filepath, delete_checkpoint)      \n",
    "        \n",
    "        print('Training finished!')\n",
    "        \n",
    "    def train_scale(self, epochs = 5, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0):\n",
    "        # Generate labels\n",
    "        label_scale, scale_vec = self.label_gen('Scale', self.df_filtered)\n",
    "        \n",
    "        # Initialize datagenerator\n",
    "        datagenerator_scale_train = self.DataGenerator(self.df_train, label_scale, 'Scale', self.rescaling, self.dim, batch_size, shuffle=True, scale_vec=scale_vec)\n",
    "        datagenerator_scale_validation = self.DataGenerator(self.df_validation, label_scale, 'Scale', self.rescaling, self.dim, batch_size, shuffle=False, scale_vec=scale_vec)\n",
    "\n",
    "        checkpoint_filepath = os.path.join(self.path_model, 'Scale')\n",
    "        \n",
    "        if not transfer_learning:\n",
    "            # Generate model\n",
    "            model_scale = self.build_model(n_classes = len(label_scale))\n",
    "\n",
    "            # Train model\n",
    "            history_scale_train = self.CNN_training(model_scale, datagenerator_scale_train, datagenerator_scale_validation, epochs, checkpoint_filepath)\n",
    "        else:\n",
    "            history_scale_train = self.ft_training(reg_id, model_id, 'Scale', datagenerator_scale_train, datagenerator_scale_validation, checkpoint_filepath, len(label_scale),lr_base = 0.001, epochs_base = 1, epochs_ft = 10)\n",
    "          \n",
    "        # Transform tensorflow SavedModel to tensorflow lite models\n",
    "        self.transform_model(checkpoint_filepath, delete_checkpoint)\n",
    "        \n",
    "        print('Training finished!')\n",
    "\n",
    "    def ft_training(self, reg_id, model_id, mode, datagenerator_train, datagenerator_validation, checkpoint_filepath, n_classes, lr_base = 0.001, epochs_base = 1, epochs_ft = 10):\n",
    "\n",
    "        # Load model for fine tuning\n",
    "\n",
    "        # Load register file\n",
    "        df_register = pd.read_csv('.\\\\data\\\\Register.csv', sep = ';', index_col = 'id')\n",
    "        # Load model register\n",
    "        df_system = df_register.loc[reg_id]\n",
    "        path_models_reg = os.path.join(df_system.loc['path'], 'models', 'Register_models.csv')\n",
    "        # Find correct model and define path\n",
    "        df_model = pd.read_csv(path_models_reg, sep = ';', index_col = 'id')\n",
    "        path_model = os.path.join(df_model.loc[model_id]['path'], mode)\n",
    "        # Load model\n",
    "        model = tf.keras.models.load_model(path_model)\n",
    "        print('Loaded model for transfer learning: ' + path_model)\n",
    "\n",
    "        # Replace last classification layer and replace with correct n_classes\n",
    "        x_ft = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "        outputs = tf.keras.layers.Dense(n_classes, activation='softmax', name = 'dense_class')(x_ft.output)\n",
    "        model_ft = tf.keras.Model(inputs=model.input, outputs=outputs)\n",
    "\n",
    "        # Freeze base model\n",
    "        model_ft.layers[1].trainable = False\n",
    "\n",
    "        # Callback functions for saving the model with the lowest validation loss --> used for final model\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=False,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True)\n",
    "        \n",
    "\n",
    "        # Compile  model\n",
    "        model_ft.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate = lr_base),\n",
    "                      metrics=[tf.keras.metrics.CategoricalAccuracy()]) \n",
    "\n",
    "        # Train fully connected layers\n",
    "        print('Transfer learning (Freezed base model).')\n",
    "        history_train = model_ft.fit(datagenerator_train,\n",
    "                                  epochs=epochs_base,\n",
    "                                  validation_data=datagenerator_validation,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                                  )\n",
    "\n",
    "        # Fine-tune from base model onwards\n",
    "        model_ft.layers[1].trainable = True\n",
    "        # Keep batch normalization frozen\n",
    "        for layer in model_ft.layers[1].layers:\n",
    "            if 'BatchNormalization' == layer.__class__.__name__:\n",
    "                layer.trainable = False\n",
    "\n",
    "        fine_tune_at = 2\n",
    "        # Freeze all the layers before the `fine_tune_at` layer\n",
    "        for layer in model_ft.layers[fine_tune_at:]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Fine tune base model (with lower learning rate)\n",
    "        model_ft.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=tf.keras.optimizers.Adam(learning_rate = lr_base),\n",
    "                      metrics=[tf.keras.metrics.CategoricalAccuracy()]) \n",
    "\n",
    "        # Train base model\n",
    "        print('Fine tuning (Unfreezed base model).')\n",
    "        history_train = model_ft.fit(datagenerator_train,\n",
    "                                  epochs=epochs_ft,\n",
    "                                  validation_data=datagenerator_validation,\n",
    "                                  callbacks=[model_checkpoint_callback]\n",
    "                                  )\n",
    "\n",
    "        return history_train\n",
    "        \n",
    "        \n",
    "        \n",
    "    def transform_model(self, path, delete_checkpoint):\n",
    "        # Convert to tensorflow lite framework\n",
    "        converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        # Save the model.\n",
    "        with open(os.path.join(path + '.tflite'), 'wb') as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        # Delete tensorflow saved model\n",
    "        if delete_checkpoint:\n",
    "            shutil.rmtree(path)\n",
    "        \n",
    "    def label_gen(self, case, dataframe):\n",
    "        # Generate labels\n",
    "        if case == 'Thickness':\n",
    "            # Number of different classes\n",
    "            label_unique = np.unique(dataframe['Thickness'])\n",
    "            # Save as dataframe\n",
    "            df_labels = pd.DataFrame({'Thickness / A' : label_unique,'Index' : np.arange(0,len(label_unique))})\n",
    "            # Only required for scaling\n",
    "            scale_vec = None\n",
    "        elif case == 'Mistilt':\n",
    "            # Number of different classes\n",
    "            label_unique = np.unique(dataframe['Mistilt'])\n",
    "            # Save as dataframe\n",
    "            df_labels = pd.DataFrame({'Mistilt / mrad' : label_unique,'Index' : np.arange(0,len(label_unique))})\n",
    "            # Only required for scaling\n",
    "            scale_vec = None\n",
    "        elif case == 'Scale':\n",
    "            # Used scaling operations\n",
    "            scale_vec = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1, 1.05, 1.1, 1.2, 1.4, 1.5, 1.6, 1.7, 1.8]\n",
    "            # Number of different classes\n",
    "            label_unique = np.unique(scale_vec)\n",
    "            # Save as dataframe\n",
    "            df_labels = pd.DataFrame({'Scale / []' : label_unique,'Index' : np.arange(0,len(label_unique))})\n",
    "            \n",
    "        # Save labels\n",
    "        df_labels.to_csv(os.path.join(self.path_model, case + '_labels.csv'), sep = ';', index=False)\n",
    "        print('Number of labels: {}'.format(len(df_labels)))\n",
    "        \n",
    "        return df_labels, scale_vec\n",
    "\n",
    "    # Make custom data generator\n",
    "    class DataGenerator(tf.keras.utils.Sequence):\n",
    "        def __init__(self, df, labels, case, rescaling, dim=(299, 299, 1), batch_size=8, shuffle=True, scale_vec=None):\n",
    "            # Declare variables\n",
    "            self.batch_size = batch_size\n",
    "            self.df = df.copy(deep=True)\n",
    "            self.indices = self.df.index.tolist()\n",
    "            self.labels = labels\n",
    "            self.num_classes = len(self.labels)\n",
    "            self.shuffle = shuffle\n",
    "            self.case = case\n",
    "            self.dim = dim\n",
    "            self.on_epoch_end()\n",
    "            self.scale_vec = scale_vec\n",
    "            self.rescaling = rescaling\n",
    "\n",
    "            # Normalize convergence angle input\n",
    "            self.conv_borders = [np.amin(self.df['Conv_Angle']), np.amax(self.df['Conv_Angle'])]\n",
    "\n",
    "            if self.conv_borders[0] - self.conv_borders[1] == 0:\n",
    "                self.df.loc[:, 'Conv_Angle_normed'] = 1\n",
    "            else:\n",
    "                self.conv_ratio = self.conv_borders[0]/(self.conv_borders[1]-self.conv_borders[0])\n",
    "                self.df['Conv_Angle_normed'] = self.df['Conv_Angle']/(self.conv_borders[1]-self.conv_borders[0]) - self.conv_ratio\n",
    "\n",
    "        def __len__(self):\n",
    "            return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            index = self.index[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "            batch = [self.indices[k] for k in index]\n",
    "\n",
    "            X, y = self.__get_data(batch)\n",
    "            return X, y\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "            self.index = np.arange(len(self.indices))\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.index)\n",
    "\n",
    "        def __get_label(self, label_id):\n",
    "            # One Hot Encoding (maybe integer encoding better suitable)\n",
    "            label_id = tf.keras.utils.to_categorical(label_id, self.num_classes)\n",
    "            return label_id\n",
    "\n",
    "        def __get_data(self, batch):\n",
    "            X = np.empty((self.batch_size, *self.dim))\n",
    "            X_conv = np.empty((self.batch_size, 1))\n",
    "            y = np.empty((self.batch_size, self.num_classes))\n",
    "\n",
    "            for i, id in enumerate(batch):\n",
    "\n",
    "                # Loading image\n",
    "                img = Image.open(self.df['Path'][id])\n",
    "                img_arr = np.array(img)\n",
    "\n",
    "                # If grayscale, add a dimension (required for tensorflow)\n",
    "                if len(img.size) == 2:\n",
    "                    img_arr = np.array(img)[:, :, np.newaxis].astype(np.float32)\n",
    "\n",
    "                # Normalize image and change datatype\n",
    "                img_arr = ((img_arr-np.amin(img_arr)) / (np.amax(img_arr)-np.amin(img_arr))).astype(np.float32)\n",
    "\n",
    "                # Resize image to the required dimension - Image already correct dimension --> save time by commenting out\n",
    "                if self.rescaling:\n",
    "                    img_arr = tf.keras.preprocessing.image.smart_resize(img_arr, self.dim[0:2], interpolation='bilinear')\n",
    "\n",
    "\n",
    "                # Loading Label for different cases (for scaling the label depends on the applied scale_rnd value)\n",
    "                if self.case == 'Thickness':\n",
    "                    y_val = self.df['Thickness'][id]\n",
    "                elif self.case == 'Mistilt':\n",
    "                    y_val = self.df['Mistilt'][id]\n",
    "                elif self.case == 'Scale':\n",
    "                    # Make a random scaling operation from the given vector\n",
    "                    scale_rnd = self.scale_vec[np.random.randint(len(self.scale_vec), size=1)[0]]\n",
    "                    # Scale image\n",
    "                    img_arr = tf.keras.preprocessing.image.apply_affine_transform(img_arr, zx=scale_rnd, zy=scale_rnd, row_axis=0, col_axis=1, channel_axis=2, fill_mode='constant', cval=0., order=1)\n",
    "                    y_val = scale_rnd\n",
    "\n",
    "                # Get categorical labels, one-hot encoded\n",
    "                y[i,] = self.__get_label(np.array(self.labels.loc[self.labels.iloc[:, 0] == y_val, 'Index']))\n",
    "\n",
    "\n",
    "                # Random scaling only if scale is not trained (equal zooming in x and y, no straining)\n",
    "                if self.case != 'Scale':\n",
    "                    # Zoom image\n",
    "                    zoom = np.random.uniform(0.8,1.2)\n",
    "                    # Allow small random stretching\n",
    "                    zoom_x = np.random.normal(zoom, 0.07)\n",
    "                    zoom_y = np.random.normal(zoom, 0.07)\n",
    "                    img_arr = tf.keras.preprocessing.image.apply_affine_transform(img_arr, zx=zoom_x, zy=zoom_y, row_axis=0, col_axis=1, channel_axis=2, fill_mode='constant', cval=0., order=1)\n",
    "\n",
    "\n",
    "                # Random shear\n",
    "                img_arr = tf.keras.preprocessing.image.random_shear(img_arr, intensity=0.05, row_axis=0, col_axis=1,\n",
    "                                                                    channel_axis=2, fill_mode='constant', cval=0.0,\n",
    "                                                                    interpolation_order=1)\n",
    "\n",
    "                # Random rotation (may change rotation from 45° to 90°)\n",
    "                img_arr = tf.keras.preprocessing.image.random_rotation(img_arr, rg=45, row_axis=0, col_axis=1,\n",
    "                                                                       channel_axis=2, fill_mode='constant', cval=0.0,\n",
    "                                                                       interpolation_order=1)\n",
    "\n",
    "                 # Random vertical and horizontal shift\n",
    "                img_arr = tf.keras.preprocessing.image.random_shift(img_arr, wrg=0.1, hrg=0.1, row_axis=0, col_axis=1,\n",
    "                                                                    channel_axis=2, fill_mode='constant', cval=0.0,\n",
    "                                                                    interpolation_order=1)           \n",
    "\n",
    "                # Random flip left/right and up/down\n",
    "                if random.choice([0, 1]):\n",
    "                    img_arr = tf.image.flip_left_right(img_arr)\n",
    "                if random.choice([0, 1]):\n",
    "                    img_arr = tf.image.flip_up_down(img_arr)\n",
    "\n",
    "\n",
    "                # Add gaussian noise\n",
    "                if self.case == 'Scale':\n",
    "                sigma = np.random.randint(50, high=100, size=None, dtype=int)/500\n",
    "                else:\n",
    "                sigma = np.random.randint(0, high=100, size=None, dtype=int)/1000\n",
    "                gauss = np.random.normal(0,sigma,img_arr.shape)\n",
    "                img_arr = np.array(img_arr + gauss).astype(np.float32)\n",
    "\n",
    "                # Normalize between -1 to 1 (due to keras input for xception model)\n",
    "                img_arr = 2*(img_arr-np.amin(img_arr))/(np.amax(img_arr)-np.amin(img_arr))-1\n",
    "\n",
    "                # Get convergence angle and normalize it\n",
    "                conv_normed = self.df['Conv_Angle_normed'][id]\n",
    "                \n",
    "                # Add gaussian noise, because user will not give exactly convergence angle\n",
    "                conv_normed = np.random.normal(conv_normed, 0.05)\n",
    "\n",
    "                # Filling batch\n",
    "                X[i,] = (np.stack((img_arr[:,:,0],)*self.dim[2], axis=-1)).astype(np.float32)\n",
    "\n",
    "                #X[i,] = img_arr\n",
    "                X_conv[i,] = np.float32(conv_normed)\n",
    "\n",
    "            return (X, X_conv), y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conservative-thought",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded system: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>material</th>\n",
       "      <th>composition</th>\n",
       "      <th>direction</th>\n",
       "      <th>thickness</th>\n",
       "      <th>thickness step</th>\n",
       "      <th>high tension</th>\n",
       "      <th>convergenc angle min</th>\n",
       "      <th>convergenc angle max</th>\n",
       "      <th>convergenc angle step</th>\n",
       "      <th>mistilt min</th>\n",
       "      <th>mistilt max</th>\n",
       "      <th>mistilt step</th>\n",
       "      <th>azimuth min</th>\n",
       "      <th>azimuth max</th>\n",
       "      <th>azimuth step</th>\n",
       "      <th>dim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.\\data\\0</td>\n",
       "      <td>Rutile</td>\n",
       "      <td>TiO2</td>\n",
       "      <td>(0, 0, 1)</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>(170, 170)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        path material composition  direction  thickness  thickness step  \\\n",
       "id                                                                        \n",
       "0   .\\data\\0   Rutile        TiO2  (0, 0, 1)        100               1   \n",
       "\n",
       "    high tension  convergenc angle min  convergenc angle max  \\\n",
       "id                                                             \n",
       "0             80                  15.0                  25.0   \n",
       "\n",
       "    convergenc angle step  mistilt min  mistilt max  mistilt step  \\\n",
       "id                                                                  \n",
       "0                     0.5            0           10             1   \n",
       "\n",
       "    azimuth min  azimuth max  azimuth step         dim  \n",
       "id                                                      \n",
       "0             0          0.5           0.1  (170, 170)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CNN_trainer = CNN_PACBED_Trainer(parameters_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "joined-bookmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will be trained from (15.0 to 25.0) mrad convergence angle.\n",
      "Model will be trained from (0.0 to 100.1) nm thickness.\n",
      "Number of images: 139986\n",
      "90% used for training, 10% used for validation\n"
     ]
    }
   ],
   "source": [
    "CNN_trainer.filter_dataset(conv_range = parameters_training['conv_range'], thickness_range = parameters_training['thickness_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer.CNN_dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-window",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CNN_trainer.train_thickness(epochs = 40, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer.train_mistilt(epochs = 20, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_trainer.train_scale(epochs = 5, batch_size = 8, delete_checkpoint = False, transfer_learning = False, reg_id = 0, model_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafe74a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
